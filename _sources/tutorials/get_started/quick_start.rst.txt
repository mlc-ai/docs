
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "tutorials/get_started/quick_start.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_tutorials_get_started_quick_start.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_get_started_quick_start.py:


.. _quick_start:

Quick Start
===========
**Authors**:
`Siyuan Feng <https://github.com/hzfengsy>`_

This tutorial is for people who are new to Apache TVM Unity. Taking an simple example
to show how to use Apache TVM Unity to compile a simple neural network.

.. GENERATED FROM PYTHON SOURCE LINES 31-38

Prepare the Neural Network Model
--------------------------------
Before we get started, let's prepare a neural network model first.
In this tutorial, to make things simple, we will defined a two-layer MLP networks
directly in this script. For people who are trying to run real models, please jump
to the next section.


.. GENERATED FROM PYTHON SOURCE LINES 38-59

.. code-block:: default


    import torch
    from torch import nn


    class MLPModel(nn.Module):
        def __init__(self):
            super(MLPModel, self).__init__()
            self.fc1 = nn.Linear(784, 256)
            self.relu1 = nn.ReLU()
            self.fc2 = nn.Linear(256, 10)

        def forward(self, x):
            x = self.fc1(x)
            x = self.relu1(x)
            x = self.fc2(x)
            return x


    torch_model = MLPModel()








.. GENERATED FROM PYTHON SOURCE LINES 60-70

Import Model into Apache TVM Unity
------------------------------------------
We choose `PyTorch FX <https://pytorch.org/docs/stable/fx.html>`_ as our frontend.
PyTorch FX is a toolkit for tracing PyTorch programs into a intermediate
representation (IR) with symbolic shape support.

.. note::
    Original PyTorch FX may not be compatible with HuggingFace Model. Please use
    `HuggingFace self-defined FX <https://huggingface.co/docs/optimum/torch_fx/overview>`_
    to trace the model.

.. GENERATED FROM PYTHON SOURCE LINES 70-77

.. code-block:: default


    from tvm import relax
    from tvm.relax.frontend.torch import from_fx
    from torch import fx

    torch_fx_model = fx.symbolic_trace(torch_model)








.. GENERATED FROM PYTHON SOURCE LINES 78-87

As the PyTorch model does not contain input information like in ONNX, we need
to provide the input information ourselves. This includes the shape and data
type of the input tensors, which are represented as a list of tuples.
Each tuple contains the shape and data type of one input tensor.

In this particular example, the shape of the input tensor is ``(1, 784)`` and
the data type is ``"float32"``. We combine the shape and data type in a tuple
like ``((1, 784), "float32")``. Then we gather all the input tuples into a list,
which looks like ``[((1, 784), "float32")]``.

.. GENERATED FROM PYTHON SOURCE LINES 87-90

.. code-block:: default


    input_info = [((1, 784), "float32")]








.. GENERATED FROM PYTHON SOURCE LINES 91-93

Use the Apache TVM Unity API to convert the PyTorch FX model into Relax Model.
And print it out to in the TVMScript Syntax

.. GENERATED FROM PYTHON SOURCE LINES 93-98

.. code-block:: default


    with torch.no_grad():
        mod = from_fx(torch_fx_model, input_info)
    mod.show()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /venv/apache-tvm-py3.8/lib/python3.8/site-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'
    To print formatted TVM script, please install the formatter 'Black':
    /venv/apache-tvm-py3.8/bin/python -m pip install "black==22.3.0" --upgrade --user
      warnings.warn(
    # from tvm.script import ir as I
    # from tvm.script import relax as R

    @I.ir_module
    class Module_0x8275110:
        @R.function
        def main(inp_0_0x8bf8a50: R.Tensor((1, 784), dtype="float32")) -> R.Tensor((1, 10), dtype="float32"):
            with R.dataflow():
                lv_0x8bf2920: R.Tensor((784, 256), dtype="float32") = R.permute_dims(metadata["relax.expr.Constant"][0], axes=None)
                lv1_0x8bf3740: R.Tensor((1, 256), dtype="float32") = R.matmul(inp_0_0x8bf8a50, lv_0x8bf2920, out_dtype="float32")
                lv2_0x8bf39e0: R.Tensor((1, 256), dtype="float32") = R.add(lv1_0x8bf3740, metadata["relax.expr.Constant"][1])
                lv3_0x8bf3c00: R.Tensor((1, 256), dtype="float32") = R.nn.relu(lv2_0x8bf39e0)
                lv4_0x8bf4310: R.Tensor((256, 10), dtype="float32") = R.permute_dims(metadata["relax.expr.Constant"][2], axes=None)
                lv5_0x8bf46d0: R.Tensor((1, 10), dtype="float32") = R.matmul(lv3_0x8bf3c00, lv4_0x8bf4310, out_dtype="float32")
                lv6_0x8bf4970: R.Tensor((1, 10), dtype="float32") = R.add(lv5_0x8bf46d0, metadata["relax.expr.Constant"][3])
                gv_0x8bf4a80: R.Tensor((1, 10), dtype="float32") = lv6_0x8bf4970
                R.output(gv_0x8bf4a80)
            return gv_0x8bf4a80

    # Metadata omitted. Use show_meta=True in script() method to show it.





.. GENERATED FROM PYTHON SOURCE LINES 99-106

Up to this point, we have successfully transformed the PyTorch FX model into a
TVM IRModule. It is important to mention that the IRModule is the central
abstraction of Apache TVM Unity, and it is utilized for subsequent transformations
and optimization processes. The IRModule has the ability to hold both high-level
graph IR (Relax) and low-level tensor IR (TensorIR). Currently, the IRModule
solely consists of Relax functions, which are marked with the `@R.function`
decorator.

.. GENERATED FROM PYTHON SOURCE LINES 108-127

Transform The Model
-------------------
Apply Optimization Transforms
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
We can apply a variety of optimization transforms to the IRModule. We have predefined
a set of optimization transforms to simplify their usage. By using the `get_pipeline`
function, we can apply the default optimization flow. By following the default path,
the following transformations will be applied in order:

- **LegalizeOps**: This transform converts the Relax operators into `call_tir` functions
  with the corresponding TensorIR Functions. After this transform, the IRModule will
  contain both Relax functions and TensorIR functions.
- **AnnotateTIROpPattern**: This transform annotates the pattern of the TensorIR functions,
  preparing them for subsequent operator fusion.
- **FoldConstant**: This pass performs constant folding, optimizing operations
  involving constants.
- **FuseOps and FuseTIR**: These two passes work together to fuse operators based on the
  patterns annotated in the previous step (AnnotateTIROpPattern). These passes transform
  both Relax functions and TensorIR functions.

.. GENERATED FROM PYTHON SOURCE LINES 127-131

.. code-block:: default


    mod = relax.get_pipeline()(mod)
    mod.show()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /venv/apache-tvm-py3.8/lib/python3.8/site-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'
    To print formatted TVM script, please install the formatter 'Black':
    /venv/apache-tvm-py3.8/bin/python -m pip install "black==22.3.0" --upgrade --user
      warnings.warn(
    # from tvm.script import ir as I
    # from tvm.script import tir as T
    # from tvm.script import relax as R

    @I.ir_module
    class Module_0x825d040:
        @T.prim_func(private=True)
        def fused_matmul1_add1_0x7732ff0(lv3_0x8e15630: T.Buffer((T.int64(1), T.int64(256)), "float32"), param_0_0x8c525e0: T.Buffer((T.int64(256), T.int64(10)), "float32"), param_1_0x8dc0940: T.Buffer((T.int64(10),), "float32"), var_T_add_intermediate_0x7c7ecf0: T.Buffer((T.int64(1), T.int64(10)), "float32")):
            T.func_attr({"tir.noalias": T.bool(True)})
            # with T.block("root"):
            var_matmul_intermediate_0x8c05e90 = T.alloc_buffer((T.int64(1), T.int64(10)))
            for i0_0x8c05f00, i1_0x8c490b0, k_0x8b90230 in T.grid(T.int64(1), T.int64(10), T.int64(256)):
                with T.block("matmul"):
                    v_i0_0x8b90270, v_i1_0x8c47300, v_k_0x8c41640 = T.axis.remap("SSR", [i0_0x8c05f00, i1_0x8c490b0, k_0x8b90230])
                    T.reads(lv3_0x8e15630[v_i0_0x8b90270, v_k_0x8c41640], param_0_0x8c525e0[v_k_0x8c41640, v_i1_0x8c47300])
                    T.writes(var_matmul_intermediate_0x8c05e90[v_i0_0x8b90270, v_i1_0x8c47300])
                    with T.init():
                        var_matmul_intermediate_0x8c05e90[v_i0_0x8b90270, v_i1_0x8c47300] = T.float32(0)
                    var_matmul_intermediate_0x8c05e90[v_i0_0x8b90270, v_i1_0x8c47300] = var_matmul_intermediate_0x8c05e90[v_i0_0x8b90270, v_i1_0x8c47300] + lv3_0x8e15630[v_i0_0x8b90270, v_k_0x8c41640] * param_0_0x8c525e0[v_k_0x8c41640, v_i1_0x8c47300]
            for ax0_0x754be20, ax1_0x754be60 in T.grid(T.int64(1), T.int64(10)):
                with T.block("T_add"):
                    v_ax0_0x84dce10, v_ax1_0x778e110 = T.axis.remap("SS", [ax0_0x754be20, ax1_0x754be60])
                    T.reads(var_matmul_intermediate_0x8c05e90[v_ax0_0x84dce10, v_ax1_0x778e110], param_1_0x8dc0940[v_ax1_0x778e110])
                    T.writes(var_T_add_intermediate_0x7c7ecf0[v_ax0_0x84dce10, v_ax1_0x778e110])
                    var_T_add_intermediate_0x7c7ecf0[v_ax0_0x84dce10, v_ax1_0x778e110] = var_matmul_intermediate_0x8c05e90[v_ax0_0x84dce10, v_ax1_0x778e110] + param_1_0x8dc0940[v_ax1_0x778e110]

        @T.prim_func(private=True)
        def fused_matmul_add_relu_0x7bba8a0(inp_0_0x8dee490: T.Buffer((T.int64(1), T.int64(784)), "float32"), param_0_0x7cd06f0: T.Buffer((T.int64(784), T.int64(256)), "float32"), param_1_0x7cd07a0: T.Buffer((T.int64(256),), "float32"), var_compute_intermediate_0x8c4bea0: T.Buffer((T.int64(1), T.int64(256)), "float32")):
            T.func_attr({"tir.noalias": T.bool(True)})
            # with T.block("root"):
            var_matmul_intermediate_0x8c57890 = T.alloc_buffer((T.int64(1), T.int64(256)))
            var_T_add_intermediate_0x7cf13b0 = T.alloc_buffer((T.int64(1), T.int64(256)))
            for i0_0x7cd0760, i1_0x7cf1420, k_0x7cf1460 in T.grid(T.int64(1), T.int64(256), T.int64(784)):
                with T.block("matmul"):
                    v_i0_0x7cf14a0, v_i1_0x8ff32d0, v_k_0x8271400 = T.axis.remap("SSR", [i0_0x7cd0760, i1_0x7cf1420, k_0x7cf1460])
                    T.reads(inp_0_0x8dee490[v_i0_0x7cf14a0, v_k_0x8271400], param_0_0x7cd06f0[v_k_0x8271400, v_i1_0x8ff32d0])
                    T.writes(var_matmul_intermediate_0x8c57890[v_i0_0x7cf14a0, v_i1_0x8ff32d0])
                    with T.init():
                        var_matmul_intermediate_0x8c57890[v_i0_0x7cf14a0, v_i1_0x8ff32d0] = T.float32(0)
                    var_matmul_intermediate_0x8c57890[v_i0_0x7cf14a0, v_i1_0x8ff32d0] = var_matmul_intermediate_0x8c57890[v_i0_0x7cf14a0, v_i1_0x8ff32d0] + inp_0_0x8dee490[v_i0_0x7cf14a0, v_k_0x8271400] * param_0_0x7cd06f0[v_k_0x8271400, v_i1_0x8ff32d0]
            for ax0_0x7a450c0, ax1_0x8c481a0 in T.grid(T.int64(1), T.int64(256)):
                with T.block("T_add"):
                    v_ax0_0x8c481e0, v_ax1_0x80fd7c0 = T.axis.remap("SS", [ax0_0x7a450c0, ax1_0x8c481a0])
                    T.reads(var_matmul_intermediate_0x8c57890[v_ax0_0x8c481e0, v_ax1_0x80fd7c0], param_1_0x7cd07a0[v_ax1_0x80fd7c0])
                    T.writes(var_T_add_intermediate_0x7cf13b0[v_ax0_0x8c481e0, v_ax1_0x80fd7c0])
                    var_T_add_intermediate_0x7cf13b0[v_ax0_0x8c481e0, v_ax1_0x80fd7c0] = var_matmul_intermediate_0x8c57890[v_ax0_0x8c481e0, v_ax1_0x80fd7c0] + param_1_0x7cd07a0[v_ax1_0x80fd7c0]
            for i0_0x8279200, i1_0x8c57940 in T.grid(T.int64(1), T.int64(256)):
                with T.block("compute"):
                    v_i0_0x8d9f100, v_i1_0x8d9f180 = T.axis.remap("SS", [i0_0x8279200, i1_0x8c57940])
                    T.reads(var_T_add_intermediate_0x7cf13b0[v_i0_0x8d9f100, v_i1_0x8d9f180])
                    T.writes(var_compute_intermediate_0x8c4bea0[v_i0_0x8d9f100, v_i1_0x8d9f180])
                    var_compute_intermediate_0x8c4bea0[v_i0_0x8d9f100, v_i1_0x8d9f180] = T.max(var_T_add_intermediate_0x7cf13b0[v_i0_0x8d9f100, v_i1_0x8d9f180], T.float32(0))

        @R.function
        def main(inp_0_0x8bf8a50: R.Tensor((1, 784), dtype="float32")) -> R.Tensor((1, 10), dtype="float32"):
            cls = Module_0x825d040
            with R.dataflow():
                lv_0x8c49e70 = R.call_tir(cls.fused_matmul_add_relu, (inp_0_0x8bf8a50, metadata["relax.expr.Constant"][0], metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((1, 256), dtype="float32"))
                lv1_0x8da12c0 = R.call_tir(cls.fused_matmul1_add1, (lv_0x8c49e70, metadata["relax.expr.Constant"][2], metadata["relax.expr.Constant"][3]), out_sinfo=R.Tensor((1, 10), dtype="float32"))
                gv_0x8bf4a80: R.Tensor((1, 10), dtype="float32") = lv1_0x8da12c0
                R.output(gv_0x8bf4a80)
            return gv_0x8bf4a80

    # Metadata omitted. Use show_meta=True in script() method to show it.





.. GENERATED FROM PYTHON SOURCE LINES 132-134

If you are only interested in the changes of the Relax functions and omit the
TensorIR functions, print the ``main`` function of the IRModule.

.. GENERATED FROM PYTHON SOURCE LINES 134-137

.. code-block:: default


    mod["main"].show()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /venv/apache-tvm-py3.8/lib/python3.8/site-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'
    To print formatted TVM script, please install the formatter 'Black':
    /venv/apache-tvm-py3.8/bin/python -m pip install "black==22.3.0" --upgrade --user
      warnings.warn(
    # from tvm.script import relax as R

    @R.function
    def main_0x85bdb30(inp_0_0x8bf8a50: R.Tensor((1, 784), dtype="float32")) -> R.Tensor((1, 10), dtype="float32"):
        R.func_attr({"global_symbol": "main"})
        with R.dataflow():
            lv_0x8c49e70 = R.call_tir(fused_matmul_add_relu, (inp_0_0x8bf8a50, metadata["relax.expr.Constant"][0], metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((1, 256), dtype="float32"))
            lv1_0x8da12c0 = R.call_tir(fused_matmul1_add1, (lv_0x8c49e70, metadata["relax.expr.Constant"][2], metadata["relax.expr.Constant"][3]), out_sinfo=R.Tensor((1, 10), dtype="float32"))
            gv_0x8bf4a80: R.Tensor((1, 10), dtype="float32") = lv1_0x8da12c0
            R.output(gv_0x8bf4a80)
        return gv_0x8bf4a80

    # Metadata omitted. Use show_meta=True in script() method to show it.





.. GENERATED FROM PYTHON SOURCE LINES 138-146

Tensor Function Optimization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Usually we apply Tensor Function Optimization after the Relax Function Optimization,
as graph transformations will changes the TIR functions.
There are different ways to apply Tensor Function Optimization, we choose ``DLight`` on
``cuda`` target in this tutorial. Note that ``DLight`` is not the only way to optimize
the Tensor Function, for other optimizations, please refer to corresponding tutorials.


.. GENERATED FROM PYTHON SOURCE LINES 146-162

.. code-block:: default


    import tvm
    from tvm import dlight as dl

    target = tvm.target.Target("cuda")

    with target:
        mod = dl.ApplyDefaultSchedule(
            dl.gpu.Matmul(),
            dl.gpu.GEMV(),
            dl.gpu.Reduction(),
            dl.gpu.GeneralReduction(),
            dl.gpu.Fallback(),
        )(mod)
    mod.show()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /venv/apache-tvm-py3.8/lib/python3.8/site-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'
    To print formatted TVM script, please install the formatter 'Black':
    /venv/apache-tvm-py3.8/bin/python -m pip install "black==22.3.0" --upgrade --user
      warnings.warn(
    # from tvm.script import ir as I
    # from tvm.script import tir as T
    # from tvm.script import relax as R

    @I.ir_module
    class Module_0x825d040:
        @T.prim_func(private=True)
        def fused_matmul1_add1_0x8eacf60(lv3_0x8e15630: T.Buffer((T.int64(1), T.int64(256)), "float32"), param_0_0x8c525e0: T.Buffer((T.int64(256), T.int64(10)), "float32"), param_1_0x8dc0940: T.Buffer((T.int64(10),), "float32"), var_T_add_intermediate_0x7c7ecf0: T.Buffer((T.int64(1), T.int64(10)), "float32")):
            T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
            # with T.block("root"):
            var_matmul_intermediate_local_0x8c3f280 = T.alloc_buffer((T.int64(1), T.int64(10)), scope="local")
            var_matmul_intermediate_rf_local_0x8e52340 = T.alloc_buffer((T.int64(16), T.int64(1), T.int64(10)), scope="local")
            for ax0_fused_0_0x8e8c9a0 in T.thread_binding(T.int64(1), thread="blockIdx.x"):
                for ax0_fused_1_0x8e8c9e0 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    for ax1_fused_1_0x8c3c5c0 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                        with T.block("matmul_rf_init"):
                            vax1_fused_1_0x8c52560 = T.axis.spatial(T.int64(16), ax1_fused_1_0x8c3c5c0)
                            v0_0x8c0dbc0 = T.axis.spatial(T.int64(10), ax0_fused_0_0x8e8c9a0 * T.int64(16) + ax0_fused_1_0x8e8c9e0)
                            T.where(ax0_fused_0_0x8e8c9a0 * T.int64(16) + ax0_fused_1_0x8e8c9e0 < T.int64(10))
                            T.reads()
                            T.writes(var_matmul_intermediate_rf_local_0x8e52340[vax1_fused_1_0x8c52560, T.int64(0), v0_0x8c0dbc0])
                            var_matmul_intermediate_rf_local_0x8e52340[vax1_fused_1_0x8c52560, T.int64(0), v0_0x8c0dbc0] = T.float32(0)
                        for ax1_fused_0_0x8e8ca20, u_0x8c3c600 in T.grid(T.int64(16), 1):
                            with T.block("matmul_rf_update"):
                                vax1_fused_1_0x82a8160 = T.axis.spatial(T.int64(16), ax1_fused_1_0x8c3c5c0)
                                v0_0x8bf3b70 = T.axis.spatial(T.int64(10), ax0_fused_0_0x8e8c9a0 * T.int64(16) + ax0_fused_1_0x8e8c9e0)
                                vax1_fused_0_0x8b3c370 = T.axis.reduce(T.int64(16), ax1_fused_0_0x8e8ca20)
                                T.where(ax0_fused_0_0x8e8c9a0 * T.int64(16) + ax0_fused_1_0x8e8c9e0 < T.int64(10))
                                T.reads(var_matmul_intermediate_rf_local_0x8e52340[vax1_fused_1_0x82a8160, T.int64(0), v0_0x8bf3b70], lv3_0x8e15630[T.int64(0), vax1_fused_0_0x8b3c370 * T.int64(16) + vax1_fused_1_0x82a8160], param_0_0x8c525e0[vax1_fused_0_0x8b3c370 * T.int64(16) + vax1_fused_1_0x82a8160, v0_0x8bf3b70])
                                T.writes(var_matmul_intermediate_rf_local_0x8e52340[vax1_fused_1_0x82a8160, T.int64(0), v0_0x8bf3b70])
                                var_matmul_intermediate_rf_local_0x8e52340[vax1_fused_1_0x82a8160, T.int64(0), v0_0x8bf3b70] = var_matmul_intermediate_rf_local_0x8e52340[vax1_fused_1_0x82a8160, T.int64(0), v0_0x8bf3b70] + lv3_0x8e15630[T.int64(0), vax1_fused_0_0x8b3c370 * T.int64(16) + vax1_fused_1_0x82a8160] * param_0_0x8c525e0[vax1_fused_0_0x8b3c370 * T.int64(16) + vax1_fused_1_0x82a8160, v0_0x8bf3b70]
                for ax1_fused_0x7c45950 in T.thread_binding(T.int64(10), thread="threadIdx.x"):
                    for ax0_0x8c4e6b0 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                        with T.block("matmul"):
                            vax1_fused_1_0x82bf690, v0_0x774c780 = T.axis.remap("RS", [ax0_0x8c4e6b0, ax1_fused_0x7c45950])
                            T.reads(var_matmul_intermediate_rf_local_0x8e52340[vax1_fused_1_0x82bf690, T.int64(0), v0_0x774c780])
                            T.writes(var_matmul_intermediate_local_0x8c3f280[T.int64(0), v0_0x774c780])
                            with T.init():
                                var_matmul_intermediate_local_0x8c3f280[T.int64(0), v0_0x774c780] = T.float32(0)
                            var_matmul_intermediate_local_0x8c3f280[T.int64(0), v0_0x774c780] = var_matmul_intermediate_local_0x8c3f280[T.int64(0), v0_0x774c780] + var_matmul_intermediate_rf_local_0x8e52340[vax1_fused_1_0x82bf690, T.int64(0), v0_0x774c780]
                for ax0_fused_0_0x8c56aa0 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    for ax0_fused_1_0x8c3f910 in range(T.int64(1)):
                        with T.block("T_add"):
                            v0_0x82a0ab0 = T.axis.spatial(T.int64(10), ax0_fused_0_0x8c56aa0 + ax0_fused_1_0x8c3f910)
                            T.where(ax0_fused_0_0x8c56aa0 + ax0_fused_1_0x8c3f910 < T.int64(10))
                            T.reads(var_matmul_intermediate_local_0x8c3f280[T.int64(0), v0_0x82a0ab0], param_1_0x8dc0940[v0_0x82a0ab0])
                            T.writes(var_T_add_intermediate_0x7c7ecf0[T.int64(0), v0_0x82a0ab0])
                            var_T_add_intermediate_0x7c7ecf0[T.int64(0), v0_0x82a0ab0] = var_matmul_intermediate_local_0x8c3f280[T.int64(0), v0_0x82a0ab0] + param_1_0x8dc0940[v0_0x82a0ab0]

        @T.prim_func(private=True)
        def fused_matmul_add_relu_0x7bb9a30(inp_0_0x8dee490: T.Buffer((T.int64(1), T.int64(784)), "float32"), param_0_0x7cd06f0: T.Buffer((T.int64(784), T.int64(256)), "float32"), param_1_0x7cd07a0: T.Buffer((T.int64(256),), "float32"), var_compute_intermediate_0x8c4bea0: T.Buffer((T.int64(1), T.int64(256)), "float32")):
            T.func_attr({"tir.is_scheduled": 1, "tir.noalias": T.bool(True)})
            # with T.block("root"):
            var_matmul_intermediate_local_0x55cc300 = T.alloc_buffer((T.int64(1), T.int64(256)), scope="local")
            var_matmul_intermediate_rf_local_0x8c42640 = T.alloc_buffer((T.int64(16), T.int64(1), T.int64(256)), scope="local")
            for ax0_fused_0_0x8c3c550 in T.thread_binding(T.int64(16), thread="blockIdx.x"):
                for ax0_fused_1_0x8c3e670 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    for ax1_fused_1_0x8c3e6f0 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                        with T.block("matmul_rf_init"):
                            vax1_fused_1_0x8e1ac70 = T.axis.spatial(T.int64(16), ax1_fused_1_0x8c3e6f0)
                            v0_0x8c53a90 = T.axis.spatial(T.int64(256), ax0_fused_0_0x8c3c550 * T.int64(16) + ax0_fused_1_0x8c3e670)
                            T.reads()
                            T.writes(var_matmul_intermediate_rf_local_0x8c42640[vax1_fused_1_0x8e1ac70, T.int64(0), v0_0x8c53a90])
                            var_matmul_intermediate_rf_local_0x8c42640[vax1_fused_1_0x8e1ac70, T.int64(0), v0_0x8c53a90] = T.float32(0)
                        for ax1_fused_0_0x8c3e6b0, u_0x8c319c0 in T.grid(T.int64(49), 1):
                            with T.block("matmul_rf_update"):
                                vax1_fused_1_0x8bf36b0 = T.axis.spatial(T.int64(16), ax1_fused_1_0x8c3e6f0)
                                v0_0x8da0720 = T.axis.spatial(T.int64(256), ax0_fused_0_0x8c3c550 * T.int64(16) + ax0_fused_1_0x8c3e670)
                                vax1_fused_0_0x8bf4640 = T.axis.reduce(T.int64(49), ax1_fused_0_0x8c3e6b0)
                                T.reads(var_matmul_intermediate_rf_local_0x8c42640[vax1_fused_1_0x8bf36b0, T.int64(0), v0_0x8da0720], inp_0_0x8dee490[T.int64(0), vax1_fused_0_0x8bf4640 * T.int64(16) + vax1_fused_1_0x8bf36b0], param_0_0x7cd06f0[vax1_fused_0_0x8bf4640 * T.int64(16) + vax1_fused_1_0x8bf36b0, v0_0x8da0720])
                                T.writes(var_matmul_intermediate_rf_local_0x8c42640[vax1_fused_1_0x8bf36b0, T.int64(0), v0_0x8da0720])
                                var_matmul_intermediate_rf_local_0x8c42640[vax1_fused_1_0x8bf36b0, T.int64(0), v0_0x8da0720] = var_matmul_intermediate_rf_local_0x8c42640[vax1_fused_1_0x8bf36b0, T.int64(0), v0_0x8da0720] + inp_0_0x8dee490[T.int64(0), vax1_fused_0_0x8bf4640 * T.int64(16) + vax1_fused_1_0x8bf36b0] * param_0_0x7cd06f0[vax1_fused_0_0x8bf4640 * T.int64(16) + vax1_fused_1_0x8bf36b0, v0_0x8da0720]
                for ax1_fused_0x8dc9470 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    for ax0_0x48575d0 in T.thread_binding(T.int64(16), thread="threadIdx.y"):
                        with T.block("matmul"):
                            vax1_fused_1_0x8c462a0 = T.axis.reduce(T.int64(16), ax0_0x48575d0)
                            v0_0x8c52340 = T.axis.spatial(T.int64(256), ax0_fused_0_0x8c3c550 * T.int64(16) + ax1_fused_0x8dc9470)
                            T.reads(var_matmul_intermediate_rf_local_0x8c42640[vax1_fused_1_0x8c462a0, T.int64(0), v0_0x8c52340])
                            T.writes(var_matmul_intermediate_local_0x55cc300[T.int64(0), v0_0x8c52340])
                            with T.init():
                                var_matmul_intermediate_local_0x55cc300[T.int64(0), v0_0x8c52340] = T.float32(0)
                            var_matmul_intermediate_local_0x55cc300[T.int64(0), v0_0x8c52340] = var_matmul_intermediate_local_0x55cc300[T.int64(0), v0_0x8c52340] + var_matmul_intermediate_rf_local_0x8c42640[vax1_fused_1_0x8c462a0, T.int64(0), v0_0x8c52340]
                for ax0_fused_0_0x8c2fbf0 in T.thread_binding(T.int64(16), thread="threadIdx.x"):
                    for ax0_fused_1_0x8d9b420 in range(T.int64(1)):
                        with T.block("compute"):
                            v0_0x8c49d90 = T.axis.spatial(T.int64(256), ax0_fused_0_0x8c3c550 * T.int64(16) + ax0_fused_0_0x8c2fbf0 + ax0_fused_1_0x8d9b420)
                            T.reads(var_matmul_intermediate_local_0x55cc300[T.int64(0), v0_0x8c49d90], param_1_0x7cd07a0[v0_0x8c49d90])
                            T.writes(var_compute_intermediate_0x8c4bea0[T.int64(0), v0_0x8c49d90])
                            var_compute_intermediate_0x8c4bea0[T.int64(0), v0_0x8c49d90] = T.max(var_matmul_intermediate_local_0x55cc300[T.int64(0), v0_0x8c49d90] + param_1_0x7cd07a0[v0_0x8c49d90], T.float32(0))

        @R.function
        def main(inp_0_0x8bf8a50: R.Tensor((1, 784), dtype="float32")) -> R.Tensor((1, 10), dtype="float32"):
            cls = Module_0x825d040
            with R.dataflow():
                lv_0x8c49e70 = R.call_tir(cls.fused_matmul_add_relu, (inp_0_0x8bf8a50, metadata["relax.expr.Constant"][0], metadata["relax.expr.Constant"][1]), out_sinfo=R.Tensor((1, 256), dtype="float32"))
                lv1_0x8da12c0 = R.call_tir(cls.fused_matmul1_add1, (lv_0x8c49e70, metadata["relax.expr.Constant"][2], metadata["relax.expr.Constant"][3]), out_sinfo=R.Tensor((1, 10), dtype="float32"))
                gv_0x8bf4a80: R.Tensor((1, 10), dtype="float32") = lv1_0x8da12c0
                R.output(gv_0x8bf4a80)
            return gv_0x8bf4a80

    # Metadata omitted. Use show_meta=True in script() method to show it.





.. GENERATED FROM PYTHON SOURCE LINES 163-168

.. note::
    The ``DLight`` framework is still under development, and currently only supports
    GPU backends with limited operators, to be specific, common operators used in LLMs.
    We would improve the framework in the future to support more operators and backends.


.. GENERATED FROM PYTHON SOURCE LINES 170-175

Compile and Run
---------------
After the optimization, we can compile the model into a TVM runtime module.
Apache TVM Unity use Relax Virtual Machine to run the model. The following code
shows how to compile the model

.. GENERATED FROM PYTHON SOURCE LINES 175-180

.. code-block:: default


    exec = relax.build(mod, target=target)
    dev = tvm.device(str(target.kind), 0)
    vm = relax.VirtualMachine(exec, dev)








.. GENERATED FROM PYTHON SOURCE LINES 181-183

Now we can run the model on the TVM runtime module. We first prepare the input
data and then invoke the TVM runtime module to get the output.

.. GENERATED FROM PYTHON SOURCE LINES 183-191

.. code-block:: default


    import numpy as np

    data = np.random.rand(1, 784).astype("float32")
    vm.set_input("main", data)
    vm.invoke_stateful("main")
    tvm_out = vm.get_outputs("main").numpy()








.. GENERATED FROM PYTHON SOURCE LINES 192-193

We can also compare the output with the PyTorch model to verify the correctness.

.. GENERATED FROM PYTHON SOURCE LINES 193-199

.. code-block:: default


    with torch.no_grad():
        torch_out = torch_model(torch.Tensor(data)).numpy()

    np.testing.assert_allclose(tvm_out, torch_out, rtol=1e-5, atol=1e-5)








.. GENERATED FROM PYTHON SOURCE LINES 200-202

Relax VM supports timing evaluation. We can use the following code to get the
timing result.

.. GENERATED FROM PYTHON SOURCE LINES 202-205

.. code-block:: default


    timing_res = vm.time_evaluator("invoke_stateful", dev)("main")
    print(timing_res)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Execution time summary:
     mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  
       0.0077       0.0077       0.0077       0.0077       0.0000                  





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 2.452 seconds)


.. _sphx_glr_download_tutorials_get_started_quick_start.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: quick_start.py <quick_start.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: quick_start.ipynb <quick_start.ipynb>`
