





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Quick Start &mdash; Apache TVM Unity 0.14.dev0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/tlcpack_theme.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/tvm-logo-square.png"/>
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="IRModule" href="ir_module.html" />
    <link rel="prev" title="Installing Apache TVM Unity" href="../install.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/apache/tvm/tree/unity/>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://mlc.ai/mlc-llm/>MLC-LLM</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://mlc.ai/>MLC-Tutorial</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   ASF
                 </button>
                 <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/security/>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  ASF
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/security/>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/tvm-logo-small.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.14.dev0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installing Apache TVM Unity</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quick Start</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#prepare-the-neural-network-model">Prepare the Neural Network Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#import-model-into-apache-tvm-unity">Import Model into Apache TVM Unity</a></li>
<li class="toctree-l2"><a class="reference internal" href="#transform-the-model">Transform The Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#apply-optimization-transforms">Apply Optimization Transforms</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tensor-function-optimization">Tensor Function Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#compile-and-run">Compile and Run</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ir_module.html">IRModule</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deep Dive</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../deep_dive/tensor_ir/index.html">TensorIR</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../reference/api/index.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/publications.html">Publications</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- Apache TVM Unity -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Quick Start</li>
    
    
      
      
        
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/docs/edit/main/docs/get_started/tutorials/quick_start.py" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>This tutorial can be used interactively with Google Colab! You can also click
<a class="reference internal" href="#sphx-glr-download-get-started-tutorials-quick-start-py"><span class="std std-ref">here</span></a> to run the Jupyter notebook locally.</p>
<a class="reference external image-reference" href="https://colab.research.google.com/github/mlc-ai/docs/blob/gh-pages/_downloads/eceb05a9badb601d2def02240aa869e9/quick_start.ipynb"><img alt="https://raw.githubusercontent.com/tlc-pack/web-data/main/images/utilities/colab_button.svg" class="align-center" src="https://raw.githubusercontent.com/tlc-pack/web-data/main/images/utilities/colab_button.svg" width="300px" /></a>
</div>
<section class="sphx-glr-example-title" id="quick-start">
<span id="sphx-glr-get-started-tutorials-quick-start-py"></span><span id="id1"></span><h1>Quick Start<a class="headerlink" href="#quick-start" title="Permalink to this heading">¶</a></h1>
<p>This tutorial is for people who are new to Apache TVM Unity. Taking an simple example
to show how to use Apache TVM Unity to compile a simple neural network.</p>
<section id="prepare-the-neural-network-model">
<h2>Prepare the Neural Network Model<a class="headerlink" href="#prepare-the-neural-network-model" title="Permalink to this heading">¶</a></h2>
<p>Before we get started, let’s prepare a neural network model first.
In this tutorial, to make things simple, we will defined a two-layer MLP networks
directly in this script. For people who are trying to run real models, please jump
to the next section.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>


<span class="k">class</span> <span class="nc">MLPModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLPModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="n">torch_model</span> <span class="o">=</span> <span class="n">MLPModel</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="import-model-into-apache-tvm-unity">
<h2>Import Model into Apache TVM Unity<a class="headerlink" href="#import-model-into-apache-tvm-unity" title="Permalink to this heading">¶</a></h2>
<p>We choose <a class="reference external" href="https://pytorch.org/docs/stable/fx.html">PyTorch FX</a> as our frontend.
PyTorch FX is a toolkit for tracing PyTorch programs into a intermediate
representation (IR) with symbolic shape support.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Original PyTorch FX may not be compatible with HuggingFace Model. Please use
<a class="reference external" href="https://huggingface.co/docs/optimum/torch_fx/overview">HuggingFace self-defined FX</a>
to trace the model.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">relax</span>
<span class="kn">from</span> <span class="nn">tvm.relax.frontend.torch</span> <span class="kn">import</span> <span class="n">from_fx</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">fx</span>

<span class="n">torch_fx_model</span> <span class="o">=</span> <span class="n">fx</span><span class="o">.</span><span class="n">symbolic_trace</span><span class="p">(</span><span class="n">torch_model</span><span class="p">)</span>
</pre></div>
</div>
<p>As the PyTorch model does not contain input information like in ONNX, we need
to provide the input information ourselves. This includes the shape and data
type of the input tensors, which are represented as a list of tuples.
Each tuple contains the shape and data type of one input tensor.</p>
<p>In this particular example, the shape of the input tensor is <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">784)</span></code> and
the data type is <code class="docutils literal notranslate"><span class="pre">&quot;float32&quot;</span></code>. We combine the shape and data type in a tuple
like <code class="docutils literal notranslate"><span class="pre">((1,</span> <span class="pre">784),</span> <span class="pre">&quot;float32&quot;)</span></code>. Then we gather all the input tuples into a list,
which looks like <code class="docutils literal notranslate"><span class="pre">[((1,</span> <span class="pre">784),</span> <span class="pre">&quot;float32&quot;)]</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">input_info</span> <span class="o">=</span> <span class="p">[((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&quot;float32&quot;</span><span class="p">)]</span>
</pre></div>
</div>
<p>Use the Apache TVM Unity API to convert the PyTorch FX model into Relax Model.
And print it out to in the TVMScript Syntax</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">mod</span> <span class="o">=</span> <span class="n">from_fx</span><span class="p">(</span><span class="n">torch_fx_model</span><span class="p">,</span> <span class="n">input_info</span><span class="p">)</span>
<span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span># from tvm.script import ir as I
# from tvm.script import relax as R

@I.ir_module
class Module:
    @R.function
    def main(inp_0: R.Tensor((1, 784), dtype=&quot;float32&quot;)) -&gt; R.Tensor((1, 10), dtype=&quot;float32&quot;):
        with R.dataflow():
            lv: R.Tensor((784, 256), dtype=&quot;float32&quot;) = R.permute_dims(metadata[&quot;relax.expr.Constant&quot;][0], axes=None)
            lv1: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.matmul(inp_0, lv, out_dtype=&quot;float32&quot;)
            lv2: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.add(lv1, metadata[&quot;relax.expr.Constant&quot;][1])
            lv3: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.nn.relu(lv2)
            lv4: R.Tensor((256, 10), dtype=&quot;float32&quot;) = R.permute_dims(metadata[&quot;relax.expr.Constant&quot;][2], axes=None)
            lv5: R.Tensor((1, 10), dtype=&quot;float32&quot;) = R.matmul(lv3, lv4, out_dtype=&quot;float32&quot;)
            lv6: R.Tensor((1, 10), dtype=&quot;float32&quot;) = R.add(lv5, metadata[&quot;relax.expr.Constant&quot;][3])
            gv: R.Tensor((1, 10), dtype=&quot;float32&quot;) = lv6
            R.output(gv)
        return gv

# Metadata omitted. Use show_meta=True in script() method to show it.
</pre></div>
</div>
<p>Up to this point, we have successfully transformed the PyTorch FX model into a
TVM IRModule. It is important to mention that the IRModule is the central
abstraction of Apache TVM Unity, and it is utilized for subsequent transformations
and optimization processes. The IRModule has the ability to hold both high-level
graph IR (Relax) and low-level tensor IR (TensorIR). Currently, the IRModule
solely consists of Relax functions, which are marked with the <cite>&#64;R.function</cite>
decorator.</p>
</section>
<section id="transform-the-model">
<h2>Transform The Model<a class="headerlink" href="#transform-the-model" title="Permalink to this heading">¶</a></h2>
<section id="apply-optimization-transforms">
<h3>Apply Optimization Transforms<a class="headerlink" href="#apply-optimization-transforms" title="Permalink to this heading">¶</a></h3>
<p>We can apply a variety of optimization transforms to the IRModule. We have predefined
a set of optimization transforms to simplify their usage. By using the <cite>get_pipeline</cite>
function, we can apply the default optimization flow. By following the default path,
the following transformations will be applied in order:</p>
<ul class="simple">
<li><p><strong>LegalizeOps</strong>: This transform converts the Relax operators into <cite>call_tir</cite> functions
with the corresponding TensorIR Functions. After this transform, the IRModule will
contain both Relax functions and TensorIR functions.</p></li>
<li><p><strong>AnnotateTIROpPattern</strong>: This transform annotates the pattern of the TensorIR functions,
preparing them for subsequent operator fusion.</p></li>
<li><p><strong>FoldConstant</strong>: This pass performs constant folding, optimizing operations
involving constants.</p></li>
<li><p><strong>FuseOps and FuseTIR</strong>: These two passes work together to fuse operators based on the
patterns annotated in the previous step (AnnotateTIROpPattern). These passes transform
both Relax functions and TensorIR functions.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mod</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">get_pipeline</span><span class="p">()(</span><span class="n">mod</span><span class="p">)</span>
<span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span># from tvm.script import ir as I
# from tvm.script import tir as T
# from tvm.script import relax as R

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def fused_matmul1_add1(lv3: T.Buffer((T.int64(1), T.int64(256)), &quot;float32&quot;), param_0: T.Buffer((T.int64(256), T.int64(10)), &quot;float32&quot;), param_1: T.Buffer((T.int64(10),), &quot;float32&quot;), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(10)), &quot;float32&quot;)):
        T.func_attr({&quot;tir.noalias&quot;: T.bool(True)})
        # with T.block(&quot;root&quot;):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(10)))
        for i0, i1, k in T.grid(T.int64(1), T.int64(10), T.int64(256)):
            with T.block(&quot;matmul&quot;):
                v_i0, v_i1, v_k = T.axis.remap(&quot;SSR&quot;, [i0, i1, k])
                T.reads(lv3[v_i0, v_k], param_0[v_k, v_i1])
                T.writes(var_matmul_intermediate[v_i0, v_i1])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1] = var_matmul_intermediate[v_i0, v_i1] + lv3[v_i0, v_k] * param_0[v_k, v_i1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(10)):
            with T.block(&quot;T_add&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1], param_1[v_ax1])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1])
                var_T_add_intermediate[v_ax0, v_ax1] = var_matmul_intermediate[v_ax0, v_ax1] + param_1[v_ax1]

    @T.prim_func(private=True)
    def fused_matmul_add_relu(inp_0: T.Buffer((T.int64(1), T.int64(784)), &quot;float32&quot;), param_0: T.Buffer((T.int64(784), T.int64(256)), &quot;float32&quot;), param_1: T.Buffer((T.int64(256),), &quot;float32&quot;), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(256)), &quot;float32&quot;)):
        T.func_attr({&quot;tir.noalias&quot;: T.bool(True)})
        # with T.block(&quot;root&quot;):
        var_matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(256)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(256)))
        for i0, i1, k in T.grid(T.int64(1), T.int64(256), T.int64(784)):
            with T.block(&quot;matmul&quot;):
                v_i0, v_i1, v_k = T.axis.remap(&quot;SSR&quot;, [i0, i1, k])
                T.reads(inp_0[v_i0, v_k], param_0[v_k, v_i1])
                T.writes(var_matmul_intermediate[v_i0, v_i1])
                with T.init():
                    var_matmul_intermediate[v_i0, v_i1] = T.float32(0)
                var_matmul_intermediate[v_i0, v_i1] = var_matmul_intermediate[v_i0, v_i1] + inp_0[v_i0, v_k] * param_0[v_k, v_i1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(256)):
            with T.block(&quot;T_add&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(var_matmul_intermediate[v_ax0, v_ax1], param_1[v_ax1])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1])
                var_T_add_intermediate[v_ax0, v_ax1] = var_matmul_intermediate[v_ax0, v_ax1] + param_1[v_ax1]
        for i0, i1 in T.grid(T.int64(1), T.int64(256)):
            with T.block(&quot;compute&quot;):
                v_i0, v_i1 = T.axis.remap(&quot;SS&quot;, [i0, i1])
                T.reads(var_T_add_intermediate[v_i0, v_i1])
                T.writes(var_compute_intermediate[v_i0, v_i1])
                var_compute_intermediate[v_i0, v_i1] = T.max(var_T_add_intermediate[v_i0, v_i1], T.float32(0))

    @R.function
    def main(inp_0: R.Tensor((1, 784), dtype=&quot;float32&quot;)) -&gt; R.Tensor((1, 10), dtype=&quot;float32&quot;):
        cls = Module
        with R.dataflow():
            lv = R.call_tir(cls.fused_matmul_add_relu, (inp_0, metadata[&quot;relax.expr.Constant&quot;][0], metadata[&quot;relax.expr.Constant&quot;][1]), out_sinfo=R.Tensor((1, 256), dtype=&quot;float32&quot;))
            lv1 = R.call_tir(cls.fused_matmul1_add1, (lv, metadata[&quot;relax.expr.Constant&quot;][2], metadata[&quot;relax.expr.Constant&quot;][3]), out_sinfo=R.Tensor((1, 10), dtype=&quot;float32&quot;))
            gv: R.Tensor((1, 10), dtype=&quot;float32&quot;) = lv1
            R.output(gv)
        return gv

# Metadata omitted. Use show_meta=True in script() method to show it.
</pre></div>
</div>
<p>If you are only interested in the changes of the Relax functions and omit the
TensorIR functions, print the <code class="docutils literal notranslate"><span class="pre">main</span></code> function of the IRModule.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mod</span><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span># from tvm.script import relax as R

@R.function
def main(inp_0: R.Tensor((1, 784), dtype=&quot;float32&quot;)) -&gt; R.Tensor((1, 10), dtype=&quot;float32&quot;):
    with R.dataflow():
        lv = R.call_tir(fused_matmul_add_relu, (inp_0, metadata[&quot;relax.expr.Constant&quot;][0], metadata[&quot;relax.expr.Constant&quot;][1]), out_sinfo=R.Tensor((1, 256), dtype=&quot;float32&quot;))
        lv1 = R.call_tir(fused_matmul1_add1, (lv, metadata[&quot;relax.expr.Constant&quot;][2], metadata[&quot;relax.expr.Constant&quot;][3]), out_sinfo=R.Tensor((1, 10), dtype=&quot;float32&quot;))
        gv: R.Tensor((1, 10), dtype=&quot;float32&quot;) = lv1
        R.output(gv)
    return gv

# Metadata omitted. Use show_meta=True in script() method to show it.
</pre></div>
</div>
</section>
<section id="tensor-function-optimization">
<h3>Tensor Function Optimization<a class="headerlink" href="#tensor-function-optimization" title="Permalink to this heading">¶</a></h3>
<p>Usually we apply Tensor Function Optimization after the Relax Function Optimization,
as graph transformations will changes the TIR functions.
There are different ways to apply Tensor Function Optimization, we choose <code class="docutils literal notranslate"><span class="pre">DLight</span></code> on
<code class="docutils literal notranslate"><span class="pre">cuda</span></code> target in this tutorial. Note that <code class="docutils literal notranslate"><span class="pre">DLight</span></code> is not the only way to optimize
the Tensor Function, for other optimizations, please refer to corresponding tutorials.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tvm</span>
<span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">dlight</span> <span class="k">as</span> <span class="n">dl</span>

<span class="n">target</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">Target</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">target</span><span class="p">:</span>
    <span class="n">mod</span> <span class="o">=</span> <span class="n">dl</span><span class="o">.</span><span class="n">ApplyDefaultSchedule</span><span class="p">(</span>
        <span class="n">dl</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">Matmul</span><span class="p">(),</span>
        <span class="n">dl</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">GEMV</span><span class="p">(),</span>
        <span class="n">dl</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">Reduction</span><span class="p">(),</span>
        <span class="n">dl</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">GeneralReduction</span><span class="p">(),</span>
        <span class="n">dl</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">Fallback</span><span class="p">(),</span>
    <span class="p">)(</span><span class="n">mod</span><span class="p">)</span>
<span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span># from tvm.script import ir as I
# from tvm.script import tir as T
# from tvm.script import relax as R

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def fused_matmul1_add1(lv3: T.Buffer((T.int64(1), T.int64(256)), &quot;float32&quot;), param_0: T.Buffer((T.int64(256), T.int64(10)), &quot;float32&quot;), param_1: T.Buffer((T.int64(10),), &quot;float32&quot;), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(10)), &quot;float32&quot;)):
        T.func_attr({&quot;tir.is_scheduled&quot;: 1, &quot;tir.noalias&quot;: T.bool(True)})
        # with T.block(&quot;root&quot;):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(10)), scope=&quot;local&quot;)
        var_matmul_intermediate_rf_local = T.alloc_buffer((T.int64(16), T.int64(1), T.int64(10)), scope=&quot;local&quot;)
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread=&quot;blockIdx.x&quot;):
            for ax0_fused_1 in T.thread_binding(T.int64(16), thread=&quot;threadIdx.x&quot;):
                for ax1_fused_1 in T.thread_binding(T.int64(16), thread=&quot;threadIdx.y&quot;):
                    with T.block(&quot;matmul_rf_init&quot;):
                        vax1_fused_1 = T.axis.spatial(T.int64(16), ax1_fused_1)
                        v0 = T.axis.spatial(T.int64(10), ax0_fused_0 * T.int64(16) + ax0_fused_1)
                        T.where(ax0_fused_0 * T.int64(16) + ax0_fused_1 &lt; T.int64(10))
                        T.reads()
                        T.writes(var_matmul_intermediate_rf_local[vax1_fused_1, T.int64(0), v0])
                        var_matmul_intermediate_rf_local[vax1_fused_1, T.int64(0), v0] = T.float32(0)
                    for ax1_fused_0, u in T.grid(T.int64(16), 1):
                        with T.block(&quot;matmul_rf_update&quot;):
                            vax1_fused_1 = T.axis.spatial(T.int64(16), ax1_fused_1)
                            v0 = T.axis.spatial(T.int64(10), ax0_fused_0 * T.int64(16) + ax0_fused_1)
                            vax1_fused_0 = T.axis.reduce(T.int64(16), ax1_fused_0)
                            T.where(ax0_fused_0 * T.int64(16) + ax0_fused_1 &lt; T.int64(10))
                            T.reads(var_matmul_intermediate_rf_local[vax1_fused_1, T.int64(0), v0], lv3[T.int64(0), vax1_fused_0 * T.int64(16) + vax1_fused_1], param_0[vax1_fused_0 * T.int64(16) + vax1_fused_1, v0])
                            T.writes(var_matmul_intermediate_rf_local[vax1_fused_1, T.int64(0), v0])
                            var_matmul_intermediate_rf_local[vax1_fused_1, T.int64(0), v0] = var_matmul_intermediate_rf_local[vax1_fused_1, T.int64(0), v0] + lv3[T.int64(0), vax1_fused_0 * T.int64(16) + vax1_fused_1] * param_0[vax1_fused_0 * T.int64(16) + vax1_fused_1, v0]
            for ax1_fused in T.thread_binding(T.int64(10), thread=&quot;threadIdx.x&quot;):
                for ax0 in T.thread_binding(T.int64(16), thread=&quot;threadIdx.y&quot;):
                    with T.block(&quot;matmul&quot;):
                        vax1_fused_1, v0 = T.axis.remap(&quot;RS&quot;, [ax0, ax1_fused])
                        T.reads(var_matmul_intermediate_rf_local[vax1_fused_1, T.int64(0), v0])
                        T.writes(var_matmul_intermediate_local[T.int64(0), v0])
                        with T.init():
                            var_matmul_intermediate_local[T.int64(0), v0] = T.float32(0)
                        var_matmul_intermediate_local[T.int64(0), v0] = var_matmul_intermediate_local[T.int64(0), v0] + var_matmul_intermediate_rf_local[vax1_fused_1, T.int64(0), v0]
            for ax0_fused_0_1 in T.thread_binding(T.int64(16), thread=&quot;threadIdx.x&quot;):
                for ax0_fused_1 in range(T.int64(1)):
                    with T.block(&quot;T_add&quot;):
                        v0 = T.axis.spatial(T.int64(10), ax0_fused_0_1 + ax0_fused_1)
                        T.where(ax0_fused_0_1 + ax0_fused_1 &lt; T.int64(10))
                        T.reads(var_matmul_intermediate_local[T.int64(0), v0], param_1[v0])
                        T.writes(var_T_add_intermediate[T.int64(0), v0])
                        var_T_add_intermediate[T.int64(0), v0] = var_matmul_intermediate_local[T.int64(0), v0] + param_1[v0]

    @T.prim_func(private=True)
    def fused_matmul_add_relu(inp_0: T.Buffer((T.int64(1), T.int64(784)), &quot;float32&quot;), param_0: T.Buffer((T.int64(784), T.int64(256)), &quot;float32&quot;), param_1: T.Buffer((T.int64(256),), &quot;float32&quot;), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(256)), &quot;float32&quot;)):
        T.func_attr({&quot;tir.is_scheduled&quot;: 1, &quot;tir.noalias&quot;: T.bool(True)})
        # with T.block(&quot;root&quot;):
        var_matmul_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(256)), scope=&quot;local&quot;)
        var_matmul_intermediate_rf_local = T.alloc_buffer((T.int64(16), T.int64(1), T.int64(256)), scope=&quot;local&quot;)
        for ax0_fused_0 in T.thread_binding(T.int64(16), thread=&quot;blockIdx.x&quot;):
            for ax0_fused_1 in T.thread_binding(T.int64(16), thread=&quot;threadIdx.x&quot;):
                for ax1_fused_1 in T.thread_binding(T.int64(16), thread=&quot;threadIdx.y&quot;):
                    with T.block(&quot;matmul_rf_init&quot;):
                        vax1_fused_1 = T.axis.spatial(T.int64(16), ax1_fused_1)
                        v0 = T.axis.spatial(T.int64(256), ax0_fused_0 * T.int64(16) + ax0_fused_1)
                        T.reads()
                        T.writes(var_matmul_intermediate_rf_local[vax1_fused_1, T.int64(0), v0])
                        var_matmul_intermediate_rf_local[vax1_fused_1, T.int64(0), v0] = T.float32(0)
                    for ax1_fused_0, u in T.grid(T.int64(49), 1):
                        with T.block(&quot;matmul_rf_update&quot;):
                            vax1_fused_1 = T.axis.spatial(T.int64(16), ax1_fused_1)
                            v0 = T.axis.spatial(T.int64(256), ax0_fused_0 * T.int64(16) + ax0_fused_1)
                            vax1_fused_0 = T.axis.reduce(T.int64(49), ax1_fused_0)
                            T.reads(var_matmul_intermediate_rf_local[vax1_fused_1, T.int64(0), v0], inp_0[T.int64(0), vax1_fused_0 * T.int64(16) + vax1_fused_1], param_0[vax1_fused_0 * T.int64(16) + vax1_fused_1, v0])
                            T.writes(var_matmul_intermediate_rf_local[vax1_fused_1, T.int64(0), v0])
                            var_matmul_intermediate_rf_local[vax1_fused_1, T.int64(0), v0] = var_matmul_intermediate_rf_local[vax1_fused_1, T.int64(0), v0] + inp_0[T.int64(0), vax1_fused_0 * T.int64(16) + vax1_fused_1] * param_0[vax1_fused_0 * T.int64(16) + vax1_fused_1, v0]
            for ax1_fused in T.thread_binding(T.int64(16), thread=&quot;threadIdx.x&quot;):
                for ax0 in T.thread_binding(T.int64(16), thread=&quot;threadIdx.y&quot;):
                    with T.block(&quot;matmul&quot;):
                        vax1_fused_1 = T.axis.reduce(T.int64(16), ax0)
                        v0 = T.axis.spatial(T.int64(256), ax0_fused_0 * T.int64(16) + ax1_fused)
                        T.reads(var_matmul_intermediate_rf_local[vax1_fused_1, T.int64(0), v0])
                        T.writes(var_matmul_intermediate_local[T.int64(0), v0])
                        with T.init():
                            var_matmul_intermediate_local[T.int64(0), v0] = T.float32(0)
                        var_matmul_intermediate_local[T.int64(0), v0] = var_matmul_intermediate_local[T.int64(0), v0] + var_matmul_intermediate_rf_local[vax1_fused_1, T.int64(0), v0]
            for ax0_fused_0_1 in T.thread_binding(T.int64(16), thread=&quot;threadIdx.x&quot;):
                for ax0_fused_1 in range(T.int64(1)):
                    with T.block(&quot;compute&quot;):
                        v0 = T.axis.spatial(T.int64(256), ax0_fused_0 * T.int64(16) + ax0_fused_0_1 + ax0_fused_1)
                        T.reads(var_matmul_intermediate_local[T.int64(0), v0], param_1[v0])
                        T.writes(var_compute_intermediate[T.int64(0), v0])
                        var_compute_intermediate[T.int64(0), v0] = T.max(var_matmul_intermediate_local[T.int64(0), v0] + param_1[v0], T.float32(0))

    @R.function
    def main(inp_0: R.Tensor((1, 784), dtype=&quot;float32&quot;)) -&gt; R.Tensor((1, 10), dtype=&quot;float32&quot;):
        cls = Module
        with R.dataflow():
            lv = R.call_tir(cls.fused_matmul_add_relu, (inp_0, metadata[&quot;relax.expr.Constant&quot;][0], metadata[&quot;relax.expr.Constant&quot;][1]), out_sinfo=R.Tensor((1, 256), dtype=&quot;float32&quot;))
            lv1 = R.call_tir(cls.fused_matmul1_add1, (lv, metadata[&quot;relax.expr.Constant&quot;][2], metadata[&quot;relax.expr.Constant&quot;][3]), out_sinfo=R.Tensor((1, 10), dtype=&quot;float32&quot;))
            gv: R.Tensor((1, 10), dtype=&quot;float32&quot;) = lv1
            R.output(gv)
        return gv

# Metadata omitted. Use show_meta=True in script() method to show it.
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">DLight</span></code> framework is still under development, and currently only supports
GPU backends with limited operators, to be specific, common operators used in LLMs.
We would improve the framework in the future to support more operators and backends.</p>
</div>
</section>
</section>
<section id="compile-and-run">
<h2>Compile and Run<a class="headerlink" href="#compile-and-run" title="Permalink to this heading">¶</a></h2>
<p>After the optimization, we can compile the model into a TVM runtime module.
Apache TVM Unity use Relax Virtual Machine to run the model. The following code
shows how to compile the model</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">exec</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">)</span>
<span class="n">dev</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">kind</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">vm</span> <span class="o">=</span> <span class="n">relax</span><span class="o">.</span><span class="n">VirtualMachine</span><span class="p">(</span><span class="n">exec</span><span class="p">,</span> <span class="n">dev</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we can run the model on the TVM runtime module. We first prepare the input
data and then invoke the TVM runtime module to get the output.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="n">tvm_data</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">dev</span><span class="p">)</span>
<span class="n">tvm_out</span> <span class="o">=</span> <span class="n">vm</span><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">](</span><span class="n">tvm_data</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
<p>We can also compare the output with the PyTorch model to verify the correctness.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">torch_out</span> <span class="o">=</span> <span class="n">torch_model</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">data</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">tvm_out</span><span class="p">,</span> <span class="n">torch_out</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</pre></div>
</div>
<p>Relax VM supports timing evaluation. We can use the following code to get the
timing result.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">timing_res</span> <span class="o">=</span> <span class="n">vm</span><span class="o">.</span><span class="n">time_evaluator</span><span class="p">(</span><span class="s2">&quot;main&quot;</span><span class="p">,</span> <span class="n">dev</span><span class="p">)(</span><span class="n">tvm_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">timing_res</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Execution time summary:
 mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)
   0.0076       0.0076       0.0076       0.0076       0.0000
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 3.594 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-get-started-tutorials-quick-start-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/baf675793174f2e9b3d5da483e35ef27/quick_start.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">quick_start.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/eceb05a9badb601d2def02240aa869e9/quick_start.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">quick_start.ipynb</span></code></a></p>
</div>
</div>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ir_module.html" class="btn btn-neutral float-right" title="IRModule" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../install.html" class="btn btn-neutral float-left" title="Installing Apache TVM Unity" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2023 Apache Software Foundation | All rights reserved</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote">Copyright © 2023 The Apache Software Foundation. Apache TVM, Apache, the Apache feather, and the Apache TVM project logo are either trademarks or registered trademarks of the Apache Software Foundation.</div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>