{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Quick Start\n**Authors**:\n[Siyuan Feng](https://github.com/hzfengsy)\n\nThis tutorial is for people who are new to Apache TVM Unity. Taking an simple example\nto show how to use Apache TVM Unity to compile a simple neural network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the Neural Network Model\nBefore we get started, let's prepare a neural network model first.\nIn this tutorial, to make things simple, we will defined a two-layer MLP networks\ndirectly in this script. For people who are trying to run real models, please jump\nto the next section.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch import nn\n\n\nclass MLPModel(nn.Module):\n    def __init__(self):\n        super(MLPModel, self).__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(256, 10)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu1(x)\n        x = self.fc2(x)\n        return x\n\n\ntorch_model = MLPModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Model into Apache TVM Unity\nWe choose [PyTorch FX](https://pytorch.org/docs/stable/fx.html) as our frontend.\nPyTorch FX is a toolkit for tracing PyTorch programs into a intermediate\nrepresentation (IR) with symbolic shape support.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Original PyTorch FX may not be compatible with HuggingFace Model. Please use\n    [HuggingFace self-defined FX](https://huggingface.co/docs/optimum/torch_fx/overview)\n    to trace the model.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm import relax\nfrom tvm.relax.frontend.torch import from_fx\nfrom torch import fx\n\ntorch_fx_model = fx.symbolic_trace(torch_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the PyTorch model does not contain input information like in ONNX, we need\nto provide the input information ourselves. This includes the shape and data\ntype of the input tensors, which are represented as a list of tuples.\nEach tuple contains the shape and data type of one input tensor.\n\nIn this particular example, the shape of the input tensor is ``(1, 784)`` and\nthe data type is ``\"float32\"``. We combine the shape and data type in a tuple\nlike ``((1, 784), \"float32\")``. Then we gather all the input tuples into a list,\nwhich looks like ``[((1, 784), \"float32\")]``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_info = [((1, 784), \"float32\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the Apache TVM Unity API to convert the PyTorch FX model into Relax Model.\nAnd print it out to in the TVMScript Syntax\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n    mod = from_fx(torch_fx_model, input_info)\nmod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Up to this point, we have successfully transformed the PyTorch FX model into a\nTVM IRModule. It is important to mention that the IRModule is the central\nabstraction of Apache TVM Unity, and it is utilized for subsequent transformations\nand optimization processes. The IRModule has the ability to hold both high-level\ngraph IR (Relax) and low-level tensor IR (TensorIR). Currently, the IRModule\nsolely consists of Relax functions, which are marked with the `@R.function`\ndecorator.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transform The Model\nApply Optimization Transforms\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nWe can apply a variety of optimization transforms to the IRModule. We have predefined\na set of optimization transforms to simplify their usage. By using the `get_pipeline`\nfunction, we can apply the default optimization flow. By following the default path,\nthe following transformations will be applied in order:\n\n- **LegalizeOps**: This transform converts the Relax operators into `call_tir` functions\n  with the corresponding TensorIR Functions. After this transform, the IRModule will\n  contain both Relax functions and TensorIR functions.\n- **AnnotateTIROpPattern**: This transform annotates the pattern of the TensorIR functions,\n  preparing them for subsequent operator fusion.\n- **FoldConstant**: This pass performs constant folding, optimizing operations\n  involving constants.\n- **FuseOps and FuseTIR**: These two passes work together to fuse operators based on the\n  patterns annotated in the previous step (AnnotateTIROpPattern). These passes transform\n  both Relax functions and TensorIR functions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mod = relax.get_pipeline()(mod)\nmod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you are only interested in the changes of the Relax functions and omit the\nTensorIR functions, print the ``main`` function of the IRModule.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mod[\"main\"].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tensor Function Optimization\nUsually we apply Tensor Function Optimization after the Relax Function Optimization,\nas graph transformations will changes the TIR functions.\nThere are different ways to apply Tensor Function Optimization, we choose ``DLight`` on\n``cuda`` target in this tutorial. Note that ``DLight`` is not the only way to optimize\nthe Tensor Function, for other optimizations, please refer to corresponding tutorials.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tvm\nfrom tvm import dlight as dl\n\ntarget = tvm.target.Target(\"cuda\")\n\nwith target:\n    mod = dl.ApplyDefaultSchedule(\n        dl.gpu.Matmul(),\n        dl.gpu.GEMV(),\n        dl.gpu.Reduction(),\n        dl.gpu.GeneralReduction(),\n        dl.gpu.Fallback(),\n    )(mod)\nmod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>The ``DLight`` framework is still under development, and currently only supports\n    GPU backends with limited operators, to be specific, common operators used in LLMs.\n    We would improve the framework in the future to support more operators and backends.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compile and Run\nAfter the optimization, we can compile the model into a TVM runtime module.\nApache TVM Unity use Relax Virtual Machine to run the model. The following code\nshows how to compile the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "exec = relax.build(mod, target=target)\ndev = tvm.device(str(target.kind), 0)\nvm = relax.VirtualMachine(exec, dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can run the model on the TVM runtime module. We first prepare the input\ndata and then invoke the TVM runtime module to get the output.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\ndata = np.random.rand(1, 784).astype(\"float32\")\nvm.set_input(\"main\", data)\nvm.invoke_stateful(\"main\")\ntvm_out = vm.get_outputs(\"main\").numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also compare the output with the PyTorch model to verify the correctness.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n    torch_out = torch_model(torch.Tensor(data)).numpy()\n\nnp.testing.assert_allclose(tvm_out, torch_out, rtol=1e-5, atol=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Relax VM supports timing evaluation. We can use the following code to get the\ntiming result.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "timing_res = vm.time_evaluator(\"invoke_stateful\", dev)(\"main\")\nprint(timing_res)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}